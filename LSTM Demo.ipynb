{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text using an LSTM Network (No libraries)\n",
    "\n",
    "## Demo\n",
    "\n",
    "We'll train an LSTM network built in pure numpy to generate Eminem lyrics. LSTMs are a fairly simple extension to neural networks, and they're behind a lot of the amazing achievements deep learning has made in the past few years.\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Recurrent nets are cool, they're useful for learning sequences of data. Input. Hidden state. Output. \n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png \"Logo Title Text 1\")\n",
    "\n",
    "It has a weight matrix that connects input to hidden state. But also a weight matrix that connects hidden state to hidden state at previous time step.\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So we could even think of it as the same feedforward network connecting to itself overtime (unrolled) since passing in\n",
    "not just input in next training iteration but input + previous hidden state\n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png \"Logo Title Text 1\")\n",
    "\n",
    "## The Problem with Recurrent Networks\n",
    "\n",
    "If we want to predict the last word in the sentence \"The grass is green\", that's totally doable. \n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png \"Logo Title Text 1\")\n",
    "\n",
    "But if we want to predict the last word in the sentence \"I am French (2000 words later) i speak fluent French\". We need to be able to remember long range dependencies. RNN's are bad at this. They forget the long term past easily.\n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png \"Logo Title Text 1\")\n",
    "\n",
    "This is called the \"Vanishing Gradient Problem\". The Gradient exponentially decays as its backpropagated\n",
    "![alt text](http://slideplayer.com/slide/5251503/16/images/6/Recurrent+Neural+Networks.jpg \"Logo Title Text 1\")\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*8JJ6sYleUtvUZR7TOyyFVg.png \"Logo Title Text 1\")\n",
    "\n",
    "There are two factors that affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen. \n",
    "\n",
    "But there exists a solution! Enter the LSTM Cell.\n",
    "\n",
    "## The LSTM Cell (Long-Short Term Memory Cell)\n",
    "\n",
    "We've placed no constraints on how our model updates, so its knowledge can change pretty chaotically: at one frame it thinks the characters are in the US, at the next frame it sees the characters eating sushi and thinks they're in Japan, and at the next frame it sees polar bears and thinks they're on Hydra Island. \n",
    "\n",
    "This chaos means information quickly transforms and vanishes, and it's difficult for the model to keep a long-term memory. So what youâ€™d like is for the network to learn how to update its beliefs (scenes without Bob shouldn't change Bob-related information, scenes with Alice should focus on gathering details about her), in a way that its knowledge of the world evolves more gently.\n",
    "\n",
    "It replaces the normal RNN cell and uses an input, forget, and output gate. As well as a cell state\n",
    "![alt text](https://www.researchgate.net/profile/Mohsen_Fayyaz/publication/306377072/figure/fig2/AS:398082849165314@1471921755580/Fig-2-An-example-of-a-basic-LSTM-cell-left-and-a-basic-RNN-cell-right-Figure.ppm \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://kijungyoon.github.io/assets/images/lstm.png \"Logo Title Text 1\")\n",
    "These gates each have their own set of weight values. The whole thing is differentiable (meaning we compute gradients and update the weights using them) so we can backprop through it\n",
    "\n",
    "We want our model to be able to know what to forget, what to remember. So when new a input comes in, the model first forgets any long-term information it decides it no longer needs. Then it learns which parts of the new input are worth using, and saves them into its long-term memory.\n",
    "\n",
    "And instead of using the full long-term memory all the time, it learns which parts to focus on instead.\n",
    "\n",
    "Basically, we need mechanisms for forgetting, remembering, and attention. That's what the LSTM cell provides us.\n",
    "\n",
    "Whereas a vanilla RNN uses one equation to update its hidden state/memory:\n",
    "![alt text](http://i.imgur.com/nT4VBPf.png \"Logo Title Text 1\")\n",
    "\n",
    "Which piece of long term memory to remember and forget? \n",
    "We'll use new input and working memory to learn remember gate. \n",
    "Which part of new data should we use and save? \n",
    "Update working memory using attention vector. \n",
    "\n",
    "- The long-term memory, is usually called the cell state, \n",
    "- The working memory, is usually called the hidden state. This is analogous to the hidden state in vanilla RNNs.\n",
    "- The remember vector, is usually called the forget gate (despite the fact that a 1 in the forget gate still means to keep the memory and a 0 still means to forget it),\n",
    "- The save vector, is usually called the input gate (as it determines how much of the input to let into the cell state), \n",
    "- The focus vector, is usually called the output gate )\n",
    "\n",
    "## Use cases\n",
    "\n",
    "\n",
    "Video \n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/mLxsbWAYIpw/0.jpg)](https://www.youtube.com/watch?time_continue=14&v=mLxsbWAYIpw)\n",
    "\n",
    "The most popular application right now is actually in natural language processing\n",
    "which involves sequential data such as words,  sentences, sound spectrogram, etc. So applications with translation,  sentiment analysis,  text generation, etc. \n",
    "\n",
    "In other less obvious areas there's also applications of lstm.  Such as for image classification (feeding each picture's pixel in row by row). And even for deepmind's deep Q Learning agents.\n",
    "\n",
    "## Other great examples\n",
    "\n",
    "Speech recognition Tensorflow - https://github.com/zzw922cn/Automatic_Speech_Recognition\n",
    "LSTM visualization - https://github.com/HendrikStrobelt/LSTMVis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps \n",
    "\n",
    "1. Build RNN class\n",
    "2. Build LSTM Cell Class\n",
    "3. Data Loading Functions\n",
    "3. Training time!\n",
    "\n",
    "![alt text](http://eric-yuan.me/wp-content/uploads/2015/06/5.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    #input (word), expected output (next word), num of words (num of recurrences), array expected outputs, learning rate\n",
    "    def __init__ (self, xs, ys, rl, eo, lr):\n",
    "        #initial input (first word)\n",
    "        self.x = np.zeros(xs)\n",
    "        #input size \n",
    "        self.xs = xs\n",
    "        #expected output (next word)\n",
    "        self.y = np.zeros(ys)\n",
    "        #output size\n",
    "        self.ys = ys\n",
    "        #weight matrix for interpreting results from LSTM cell (num words x num words matrix)\n",
    "        self.w = np.random.random((ys, ys))\n",
    "        #matrix used in RMSprop\n",
    "        self.G = np.zeros_like(self.w)\n",
    "        #length of the recurrent network - number of recurrences i.e num of words\n",
    "        self.rl = rl\n",
    "        #learning rate \n",
    "        self.lr = lr\n",
    "        #array for storing inputs\n",
    "        self.ia = np.zeros((rl+1,xs))\n",
    "        #array for storing cell states\n",
    "        self.ca = np.zeros((rl+1,ys))\n",
    "        #array for storing outputs\n",
    "        self.oa = np.zeros((rl+1,ys))\n",
    "        #array for storing hidden states\n",
    "        self.ha = np.zeros((rl+1,ys))\n",
    "        #forget gate \n",
    "        self.af = np.zeros((rl+1,ys))\n",
    "        #input gate\n",
    "        self.ai = np.zeros((rl+1,ys))\n",
    "        #cell state\n",
    "        self.ac = np.zeros((rl+1,ys))\n",
    "        #output gate\n",
    "        self.ao = np.zeros((rl+1,ys))\n",
    "        #array of expected output values\n",
    "        self.eo = np.vstack((np.zeros(eo.shape[0]), eo.T))\n",
    "        #declare LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "        self.LSTM = LSTM(xs, ys, rl, lr)\n",
    "    \n",
    "    #activation function. simple nonlinearity, convert nums into probabilities between 0 and 1\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #the derivative of the sigmoid function. used to compute gradients for backpropagation\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))    \n",
    "    \n",
    "    #lets apply a series of matrix operations to our input (curr word) to compute a predicted output (next word)\n",
    "    def forwardProp(self):\n",
    "        for i in range(1, self.rl+1):\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            #store computed cell state\n",
    "            self.ca[i] = cs\n",
    "            self.ha[i] = hs\n",
    "            self.af[i] = f\n",
    "            self.ai[i] = inp\n",
    "            self.ac[i] = c\n",
    "            self.ao[i] = o\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            self.x = self.eo[i-1]\n",
    "        return self.oa\n",
    "   \n",
    "    \n",
    "    def backProp(self):\n",
    "        #update our weight matrices (Both in our Recurrent network, as well as the weight matrices inside LSTM cell)\n",
    "        #init an empty error value \n",
    "        totalError = 0\n",
    "        #initialize matrices for gradient updates\n",
    "        #First, these are RNN level gradients\n",
    "        #cell state\n",
    "        dfcs = np.zeros(self.ys)\n",
    "        #hidden state,\n",
    "        dfhs = np.zeros(self.ys)\n",
    "        #weight matrix\n",
    "        tu = np.zeros((self.ys,self.ys))\n",
    "        #Next, these are LSTM level gradients\n",
    "        #forget gate\n",
    "        tfu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #input gate\n",
    "        tiu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #cell unit\n",
    "        tcu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #output gate\n",
    "        tou = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #loop backwards through recurrences\n",
    "        for i in range(self.rl, -1, -1):\n",
    "            #error = calculatedOutput - expectedOutput\n",
    "            error = self.oa[i] - self.eo[i]\n",
    "            #calculate update for weight matrix\n",
    "            #(error * derivative of the output) * hidden state\n",
    "            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n",
    "            #Time to propagate error back to exit of LSTM cell\n",
    "            #1. error * RNN weight matrix\n",
    "            error = np.dot(error, self.w)\n",
    "            #2. set input values of LSTM cell for recurrence i (horizontal stack of arrays, hidden + input)\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n",
    "            #3. set cell state of LSTM cell for recurrence i (pre-updates)\n",
    "            self.LSTM.cs = self.ca[i]\n",
    "            #Finally, call the LSTM cell's backprop, retreive gradient updates\n",
    "            #gradient updates for forget, input, cell unit, and output gates + cell states & hiddens states\n",
    "            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n",
    "            #calculate total error (not necesarry, used to measure training progress)\n",
    "            totalError += np.sum(error)\n",
    "            #accumulate all gradient updates\n",
    "            #forget gate\n",
    "            tfu += fu\n",
    "            #input gate\n",
    "            tiu += iu\n",
    "            #cell state\n",
    "            tcu += cu\n",
    "            #output gate\n",
    "            tou += ou\n",
    "        #update LSTM matrices with average of accumulated gradient updates    \n",
    "        self.LSTM.update(tfu/self.rl, tiu/self.rl, tcu/self.rl, tou/self.rl) \n",
    "        #update weight matrix with average of accumulated gradient updates  \n",
    "        self.update(tu/self.rl)\n",
    "        #return total error of this iteration\n",
    "        return totalError\n",
    "    \n",
    "    def update(self, u):\n",
    "        #vanilla implementation of RMSprop\n",
    "        self.G = 0.9 * self.G + 0.1 * u**2  \n",
    "        self.w -= self.lr/np.sqrt(self.G + 1e-8) * u\n",
    "        return\n",
    "    \n",
    "    #this is where we generate some sample text after having fully trained our model\n",
    "    #i.e error is below some threshold\n",
    "    def sample(self):\n",
    "        #loop through recurrences - start at 1 so the 0th entry of all arrays will be an array of 0's\n",
    "        for i in range(1, self.rl+1):\n",
    "            #set input for LSTM cell, combination of input (previous output) and previous hidden state\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            #run forward prop on the LSTM cell, retrieve cell state and hidden state\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            #store input as vector\n",
    "            maxI = np.argmax(self.x)\n",
    "            self.x = np.zeros_like(self.x)\n",
    "            self.x[maxI] = 1\n",
    "            self.ia[i] = self.x #Use np.argmax?\n",
    "            #store cell states\n",
    "            self.ca[i] = cs\n",
    "            #store hidden state\n",
    "            self.ha[i] = hs\n",
    "            #forget gate\n",
    "            self.af[i] = f\n",
    "            #input gate\n",
    "            self.ai[i] = inp\n",
    "            #cell state\n",
    "            self.ac[i] = c\n",
    "            #output gate\n",
    "            self.ao[i] = o\n",
    "            #calculate output by multiplying hidden state with weight matrix\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            #compute new input\n",
    "            maxI = np.argmax(self.oa[i])\n",
    "            newX = np.zeros_like(self.x)\n",
    "            newX[maxI] = 1\n",
    "            self.x = newX\n",
    "        #return all outputs    \n",
    "        return self.oa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://i.imgur.com/BUAVEZg.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    # LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "    def __init__ (self, xs, ys, rl, lr):\n",
    "        #input is word length x word length\n",
    "        self.x = np.zeros(xs+ys)\n",
    "        #input size is word length + word length\n",
    "        self.xs = xs + ys\n",
    "        #output \n",
    "        self.y = np.zeros(ys)\n",
    "        #output size\n",
    "        self.ys = ys\n",
    "        #cell state intialized as size of prediction\n",
    "        self.cs = np.zeros(ys)\n",
    "        #how often to perform recurrence\n",
    "        self.rl = rl\n",
    "        #balance the rate of training (learning rate)\n",
    "        self.lr = lr\n",
    "        #init weight matrices for our gates\n",
    "        #forget gate\n",
    "        self.f = np.random.random((ys, xs+ys))\n",
    "        #input gate\n",
    "        self.i = np.random.random((ys, xs+ys))\n",
    "        #cell state\n",
    "        self.c = np.random.random((ys, xs+ys))\n",
    "        #output gate\n",
    "        self.o = np.random.random((ys, xs+ys))\n",
    "        #forget gate gradient\n",
    "        self.Gf = np.zeros_like(self.f)\n",
    "        #input gate gradient\n",
    "        self.Gi = np.zeros_like(self.i)\n",
    "        #cell state gradient\n",
    "        self.Gc = np.zeros_like(self.c)\n",
    "        #output gate gradient\n",
    "        self.Go = np.zeros_like(self.o)\n",
    "    \n",
    "    #activation function to activate our forward prop, just like in any type of neural network\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #derivative of sigmoid to help computes gradients\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    #tanh! another activation function, often used in LSTM cells\n",
    "    #Having stronger gradients: since data is centered around 0, \n",
    "    #the derivatives are higher. To see this, calculate the derivative \n",
    "    #of the tanh function and notice that input values are in the range [0,1].\n",
    "    def tangent(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    #derivative for computing gradients\n",
    "    def dtangent(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    #lets compute a series of matrix multiplications to convert our input into our output\n",
    "    def forwardProp(self):\n",
    "        f = self.sigmoid(np.dot(self.f, self.x))\n",
    "        self.cs *= f\n",
    "        i = self.sigmoid(np.dot(self.i, self.x))\n",
    "        c = self.tangent(np.dot(self.c, self.x))\n",
    "        self.cs += i * c\n",
    "        o = self.sigmoid(np.dot(self.o, self.x))\n",
    "        self.y = o * self.tangent(self.cs)\n",
    "        return self.cs, self.y, f, i, c, o\n",
    "    \n",
    "   \n",
    "    def backProp(self, e, pcs, f, i, c, o, dfcs, dfhs):\n",
    "        #error = error + hidden state derivative. clip the value between -6 and 6.\n",
    "        e = np.clip(e + dfhs, -6, 6)\n",
    "        #multiply error by activated cell state to compute output derivative\n",
    "        do = self.tangent(self.cs) * e\n",
    "        #output update = (output deriv * activated output) * input\n",
    "        ou = np.dot(np.atleast_2d(do * self.dtangent(o)).T, np.atleast_2d(self.x))\n",
    "        #derivative of cell state = error * output * deriv of cell state + deriv cell\n",
    "        dcs = np.clip(e * o * self.dtangent(self.cs) + dfcs, -6, 6)\n",
    "        #deriv of cell = deriv cell state * input\n",
    "        dc = dcs * i\n",
    "        #cell update = deriv cell * activated cell * input\n",
    "        cu = np.dot(np.atleast_2d(dc * self.dtangent(c)).T, np.atleast_2d(self.x))\n",
    "        #deriv of input = deriv cell state * cell\n",
    "        di = dcs * c\n",
    "        #input update = (deriv input * activated input) * input\n",
    "        iu = np.dot(np.atleast_2d(di * self.dsigmoid(i)).T, np.atleast_2d(self.x))\n",
    "        #deriv forget = deriv cell state * all cell states\n",
    "        df = dcs * pcs\n",
    "        #forget update = (deriv forget * deriv forget) * input\n",
    "        fu = np.dot(np.atleast_2d(df * self.dsigmoid(f)).T, np.atleast_2d(self.x))\n",
    "        #deriv cell state = deriv cell state * forget\n",
    "        dpcs = dcs * f\n",
    "        #deriv hidden state = (deriv cell * cell) * output + deriv output * output * output deriv input * input * output + deriv forget\n",
    "        #* forget * output\n",
    "        dphs = np.dot(dc, self.c)[:self.ys] + np.dot(do, self.o)[:self.ys] + np.dot(di, self.i)[:self.ys] + np.dot(df, self.f)[:self.ys] \n",
    "        #return update gradinets for forget, input, cell, output, cell state, hidden state\n",
    "        return fu, iu, cu, ou, dpcs, dphs\n",
    "            \n",
    "    def update(self, fu, iu, cu, ou):\n",
    "        #update forget, input, cell, and output gradients\n",
    "        self.Gf = 0.9 * self.Gf + 0.1 * fu**2 \n",
    "        self.Gi = 0.9 * self.Gi + 0.1 * iu**2   \n",
    "        self.Gc = 0.9 * self.Gc + 0.1 * cu**2   \n",
    "        self.Go = 0.9 * self.Go + 0.1 * ou**2   \n",
    "        \n",
    "        #update our gates using our gradients\n",
    "        self.f -= self.lr/np.sqrt(self.Gf + 1e-8) * fu\n",
    "        self.i -= self.lr/np.sqrt(self.Gi + 1e-8) * iu\n",
    "        self.c -= self.lr/np.sqrt(self.Gc + 1e-8) * cu\n",
    "        self.o -= self.lr/np.sqrt(self.Go + 1e-8) * ou\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadText():\n",
    "    #open text and return input and output data (series of words)\n",
    "    with open(\"eminem.txt\", \"r\") as text_file:\n",
    "        data = text_file.read()\n",
    "    text = list(data)\n",
    "    outputSize = len(text)\n",
    "    data = list(set(text))\n",
    "    uniqueWords, dataSize = len(data), len(data) \n",
    "    returnData = np.zeros((uniqueWords, dataSize))\n",
    "    for i in range(0, dataSize):\n",
    "        returnData[i][i] = 1\n",
    "    returnData = np.append(returnData, np.atleast_2d(data), axis=0)\n",
    "    output = np.zeros((uniqueWords, outputSize))\n",
    "    for i in range(0, outputSize):\n",
    "        index = np.where(np.asarray(data) == text[i])\n",
    "        output[:,i] = returnData[0:-1,index[0]].astype(float).ravel()  \n",
    "    return returnData, uniqueWords, output, outputSize, data\n",
    "\n",
    "#write the predicted output (series of words) to disk\n",
    "def ExportText(output, data):\n",
    "    finalOutput = np.zeros_like(output)\n",
    "    prob = np.zeros_like(output[0])\n",
    "    outputText = \"\"\n",
    "    print(len(data))\n",
    "    print(output.shape[0])\n",
    "    for i in range(0, output.shape[0]):\n",
    "        for j in range(0, output.shape[1]):\n",
    "            prob[j] = output[i][j] / np.sum(output[i])\n",
    "        outputText += np.random.choice(data, p=prob)    \n",
    "    with open(\"output.txt\", \"w\") as text_file:\n",
    "        text_file.write(outputText)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning\n",
      "Done Reading\n",
      "Error on iteration  1 :  4934422.698642616\n",
      "Error on iteration  2 :  4905103.308782349\n",
      "Error on iteration  3 :  4882868.742403485\n",
      "Error on iteration  4 :  4864223.3416419495\n",
      "Error on iteration  5 :  4847673.065214139\n",
      "Error on iteration  6 :  4832507.1639443515\n",
      "Error on iteration  7 :  4818329.503846626\n",
      "Error on iteration  8 :  4804893.650320478\n",
      "Error on iteration  9 :  4792034.5555659495\n",
      "Error on iteration  10 :  4779635.659879818\n",
      "Error on iteration  11 :  4767611.294464666\n",
      "Error on iteration  12 :  4755896.507454919\n",
      "Error on iteration  13 :  4744440.816979296\n",
      "Error on iteration  14 :  4733204.186748157\n",
      "Error on iteration  15 :  4722154.329543268\n",
      "Error on iteration  16 :  4711264.840240216\n",
      "Error on iteration  17 :  4700513.866748693\n",
      "Error on iteration  18 :  4689883.141015322\n",
      "Error on iteration  19 :  4679357.257734662\n",
      "Error on iteration  20 :  4668923.127577035\n",
      "Error on iteration  21 :  4658569.555985225\n",
      "Error on iteration  22 :  4648286.914012297\n",
      "Error on iteration  23 :  4638066.877763362\n",
      "Error on iteration  24 :  4627902.219743064\n",
      "Error on iteration  25 :  4617786.64001126\n",
      "Error on iteration  26 :  4607714.628252447\n",
      "Error on iteration  27 :  4597681.3501217915\n",
      "Error on iteration  28 :  4587682.552856434\n",
      "Error on iteration  29 :  4577714.486323059\n",
      "Error on iteration  30 :  4567773.836543331\n",
      "Error on iteration  31 :  4557857.669388812\n",
      "Error on iteration  32 :  4547963.382627759\n",
      "Error on iteration  33 :  4538088.664881721\n",
      "Error on iteration  34 :  4528231.460332264\n",
      "Error on iteration  35 :  4518389.93824796\n",
      "Error on iteration  36 :  4508562.466570334\n",
      "Error on iteration  37 :  4498747.588941119\n",
      "Error on iteration  38 :  4488944.004654142\n",
      "Error on iteration  39 :  4479150.551114237\n",
      "Error on iteration  40 :  4469366.188444449\n",
      "Error on iteration  41 :  4459589.9859488625\n",
      "Error on iteration  42 :  4449821.110180184\n",
      "Error on iteration  43 :  4440058.814400156\n",
      "Error on iteration  44 :  4430302.42925535\n",
      "Error on iteration  45 :  4420551.354512728\n",
      "Error on iteration  46 :  4410805.051725011\n",
      "Error on iteration  47 :  4401063.037711776\n",
      "Error on iteration  48 :  4391324.878757939\n",
      "Error on iteration  49 :  4381590.185447103\n",
      "Error on iteration  50 :  4371858.60805308\n",
      "Error on iteration  51 :  4362129.832426707\n",
      "Error on iteration  52 :  4352403.576322762\n",
      "Error on iteration  53 :  4342679.586115496\n",
      "Error on iteration  54 :  4332957.633861187\n",
      "Error on iteration  55 :  4323237.514668844\n",
      "Error on iteration  56 :  4313519.0443453025\n",
      "Error on iteration  57 :  4303802.057285864\n",
      "Error on iteration  58 :  4294086.404582693\n",
      "Error on iteration  59 :  4284371.9523295\n",
      "Error on iteration  60 :  4274658.580100375\n",
      "Error on iteration  61 :  4264946.179583837\n",
      "Error on iteration  62 :  4255234.653358392\n",
      "Error on iteration  63 :  4245523.913792411\n",
      "Error on iteration  64 :  4235813.882055606\n",
      "Error on iteration  65 :  4226104.4872312825\n",
      "Error on iteration  66 :  4216395.665520353\n",
      "Error on iteration  67 :  4206687.359523721\n",
      "Error on iteration  68 :  4196979.517598163\n",
      "Error on iteration  69 :  4187272.093278628\n",
      "Error on iteration  70 :  4177565.044757225\n",
      "Error on iteration  71 :  4167858.334415312\n",
      "Error on iteration  72 :  4158151.928403492\n",
      "Error on iteration  73 :  4148445.796262881\n",
      "Error on iteration  74 :  4138739.9105849126\n",
      "Error on iteration  75 :  4129034.246705902\n",
      "Error on iteration  76 :  4119328.782430876\n",
      "Error on iteration  77 :  4109623.497787577\n",
      "Error on iteration  78 :  4099918.374802049\n",
      "Error on iteration  79 :  4090213.397300171\n",
      "Error on iteration  80 :  4080508.550725804\n",
      "Error on iteration  81 :  4070803.821979433\n",
      "Error on iteration  82 :  4061099.1992727355\n",
      "Error on iteration  83 :  4051394.6719960477\n",
      "Error on iteration  84 :  4041690.2306020297\n",
      "Error on iteration  85 :  4031985.8664978016\n",
      "Error on iteration  86 :  4022281.5719499257\n",
      "Error on iteration  87 :  4012577.339999314\n",
      "Error on iteration  88 :  4002873.164381698\n",
      "Error on iteration  89 :  3993169.0394605054\n",
      "Error on iteration  90 :  3983464.9601618513\n",
      "Error on iteration  91 :  3973760.921919951\n",
      "Error on iteration  92 :  3964056.920625116\n",
      "Error on iteration  93 :  3954352.952578629\n",
      "Error on iteration  94 :  3944649.0144516598\n",
      "Error on iteration  95 :  3934945.1032480923\n",
      "Error on iteration  96 :  3925241.21627134\n",
      "Error on iteration  97 :  3915537.351093978\n",
      "Error on iteration  98 :  3905833.505531643\n",
      "Error on iteration  99 :  3896129.6776179792\n",
      "Error on iteration  100 :  3886425.8655829052\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.9999867  0.9999895  0.99999731 ... 0.99999846 0.99998187 0.999991  ]\n",
      " [0.99999999 0.99999999 1.         ... 1.         0.99999998 0.99999999]\n",
      " ...\n",
      " [0.99999999 0.99999999 1.         ... 1.         0.99999998 0.99999999]\n",
      " [0.99999999 0.99999999 1.         ... 1.         0.99999998 0.99999999]\n",
      " [0.99999999 0.99999999 1.         ... 1.         0.99999998 0.99999999]]\n",
      "52\n",
      "3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amiga\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8e94ec71b30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#write it all to disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mExportText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done Writing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-09ea750e3f39>\u001b[0m in \u001b[0;36mExportText\u001b[1;34m(output, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0moutputText\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "#Begin program    \n",
    "print(\"Beginning\")\n",
    "iterations = 5000\n",
    "learningRate = 0.001\n",
    "#load input output data (words)\n",
    "returnData, numCategories, expectedOutput, outputSize, data = LoadText()\n",
    "print(\"Done Reading\")\n",
    "#init our RNN using our hyperparams and dataset\n",
    "RNN = RecurrentNeuralNetwork(numCategories, numCategories, outputSize, expectedOutput, learningRate)\n",
    "\n",
    "#training time!\n",
    "for i in range(1, iterations):\n",
    "    #compute predicted next word\n",
    "    RNN.forwardProp()\n",
    "    #update all our weights using our error\n",
    "    error = RNN.backProp()\n",
    "    #once our error/loss is small enough\n",
    "    print(\"Error on iteration \", i, \": \", error)\n",
    "    if error > -100 and error < 100 or i % 100 == 0:\n",
    "        #we can finally define a seed word\n",
    "        seed = np.zeros_like(RNN.x)\n",
    "        maxI = np.argmax(np.random.random(RNN.x.shape))\n",
    "        seed[maxI] = 1\n",
    "        RNN.x = seed  \n",
    "        #and predict some new text!\n",
    "        output = RNN.sample()\n",
    "        print(output)    \n",
    "        #write it all to disk\n",
    "        ExportText(output, data)\n",
    "        print(\"Done Writing\")\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
